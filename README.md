A large-scale benchmark called ScenEval, consisting of 12,864 tasks for testing LLMs on their capability at generating Java programs from natural language description of coding tasks, was constructed by extracting sources automatically from professional problem-solving forum Stack Overflow and online training website W3Resource Tutorial on Java programming, and manually from textbooks of Java programming. The former forms a sample of real-world coding problems asked by professional developers while the latter forms a sample of textbook questions that are presented well by expert and authoritative authors. There are also a number of novelties in the design of the benchmark, which include the use of metadata to describe the scenarios of the coding tasks in the benchmark, and the inclusion of reference solutions of the coding tasks.
![fig43](https://github.com/user-attachments/assets/9dbe9891-e992-47c6-b561-8183aa1e7eff)
![fig_4](https://github.com/user-attachments/assets/b0c16ce1-25e0-41fb-83cf-e2026e7fd5af)


![fig_5](https://github.com/user-attachments/assets/af887814-199c-4cb7-8553-5a2d9154587b)
