A large-scale benchmark called ScenEval, consisting of 12,864 elements for testing LLMs on their capability at generating Java programs from natural language description of coding tasks, was constructed by extracting sources automatically from professional problem-solving forum Stack Overflow and online training website W3Resource Tutorial on Java programming, and manually from textbooks of Java programming. The former forms a sample of real-world coding problems asked by professional developers while the latter forms a sample of textbook questions that are presented well by expert and authoritative authors. There are also a number of novelties in the design of the benchmark, which include the use of metadata to describe the scenarios of the coding tasks in the benchmark, and the inclusion of reference solutions of the coding tasks. A test system is also designed and implemented to support the use of the benchmark in the testing and evaluation of LLMs and in the analysis of test results in various combinations of scenarios. This provides a solid base for implementing test automation systems for various topic-specific testing techniques and methods.
